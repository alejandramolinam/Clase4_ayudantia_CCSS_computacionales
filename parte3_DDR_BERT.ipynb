{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers\n",
    "#%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip3 install pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##imports##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from random import sample \n",
    "from numpyencoder import NumpyEncoder\n",
    "import math\n",
    "from unicodedata import normalize\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend(['i','ii','iii','iv','v','vi','vii', 'ix' , 'x' , 'xi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "nlp  =  spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test con Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el modelo pre entrenado BERT para calcular los vectores con word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT (PRUEBA)\n",
    "\n",
    "# Import generic wrappers\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "# Define the model repo\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\" \n",
    "\n",
    "# Download pytorch model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de uso:\n",
    "\n",
    "Primero se tokeniza el documento con el \"tokenizador\" pre entrenado de BERT. Después se aplica el modelo pre entrenado de word embedding a la frase \"tokenizada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform input tokens \n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# Model apply\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas Funciones útiles\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True,max_len = 22))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc),max_len = 22,deacc = True) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV','PROPN','X']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out  =  []\n",
    "    for sent in texts:\n",
    "        doc  =  nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def make_agg_vec(words, model, num_features, model_word_set, filter_out = []):\n",
    "    \"\"\"Create aggregate representation of list of words\"\"\"\n",
    "    feature_vec  =  np.zeros((num_features,), dtype = \"float32\")\n",
    "    nwords  =  0.0\n",
    "    lost_words = []\n",
    "    for word in words:\n",
    "        if word not in filter_out:\n",
    "            if word in model_word_set:\n",
    "                nwords += 1\n",
    "                feature_vec = np.add(feature_vec , model[word])\n",
    "            else:\n",
    "                lost_words.append(word)\n",
    "    if nwords>0:\n",
    "        avg_feature_vec = feature_vec / nwords\n",
    "        return [avg_feature_vec,lost_words]\n",
    "    else:\n",
    "      return [feature_vec,lost_words]\n",
    "\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "def cos_similarity_distance(v1, v2):\n",
    " return 1-cos_similarity(v1,v2)\n",
    "\n",
    "\n",
    "def cos_similarity_angle(v1, v2):\n",
    "    prod  =  np.dot(v1, v2)\n",
    "    len1  =  math.sqrt(np.dot(v1, v1))\n",
    "    len2  =  math.sqrt(np.dot(v2, v2))\n",
    "    if   abs(prod / (len1 * len2) -1)<(10**(-4)):\n",
    "        return 0\n",
    "    if len1*len2 == 0:\n",
    "        return -1\n",
    "    else:\n",
    "      return   math.degrees(    math.acos( prod / (len1 * len2))  )# retorna un angulo entre 0 y 180\n",
    "    \n",
    "\n",
    "\n",
    "def cos_similarity_pearson(v1, v2):\n",
    "    v1    = v1-np.mean(v1)\n",
    "    v2    = v2-np.mean(v2)\n",
    "    prod  =  np.dot(v1, v2)\n",
    "    len1  =  math.sqrt(np.dot(v1, v1))\n",
    "    len2  =  math.sqrt(np.dot(v2, v2))\n",
    "    if len1*len2 == 0:\n",
    "        return -1\n",
    "    else:\n",
    "      return   prod / (len1 * len2)  # retorna un angulo entre 0 y 180\n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    \"\"\" Preprocesar el documento eliminando stopwords, lematizando, eliminando caracteres no alfanuméricos y palabras vacías. \"\"\"\n",
    "    # Asegurarse de que el documento sea una cadena\n",
    "    if isinstance(doc, list):\n",
    "        doc = ' '.join(doc)\n",
    "    # Eliminar stopwords y lematizar\n",
    "    # doc = nlp(doc)\n",
    "    # lemmatized = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # print(doc)\n",
    "    no_stopw=remove_stopwords([doc])\n",
    "    # print(no_stopw)\n",
    "    lemmatized=lemmatization(no_stopw)\n",
    "    # Eliminar caracteres no alfanuméricos, convertir a minúsculas y eliminar palabras vacías\n",
    "    # cleaned = [re.sub(r'\\W+', '', word).lower() for word in lemmatized]\n",
    "    cleaned = [word for word in lemmatized]  # Elimina las palabras vacías\n",
    "    cleaned=cleaned[0]\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_bert_vec(doc, model, tokenizer):\n",
    "    \"\"\"Generate BERT embedding for a single document.\"\"\"\n",
    "    encoded_input = tokenizer(doc, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    # Promedio de todos los tokens para obtener un vector único para el documento\n",
    "    mean_embedding = output.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return mean_embedding.numpy()\n",
    "\n",
    "# Ejemplo de uso\n",
    "testmessage = \"hola, me llamo antonio y estoy ansioso por comenzar el curso\"\n",
    "bert_vector = make_bert_vec(preprocess_document(testmessage), model, tokenizer)\n",
    "\n",
    "\n",
    "def make_bert_cls_vec(doc, model, tokenizer):\n",
    "    \"\"\"Generate the BERT [CLS] embedding for a single document.\n",
    "       \n",
    "       Para utilizar el vector [CLS] de BERT para tareas de similitud, modificaremos la función de generación de vectores \n",
    "       para que devuelva específicamente el vector [CLS] en lugar del promedio de todos los vectores de token. El token [CLS] \n",
    "       se encuentra al principio de cada secuencia y se entrena para representar el significado agregado de toda la entrada, \n",
    "       lo que lo hace útil para tareas de comparación de documentos.\"\"\"\n",
    "    \n",
    "    # Tokenizar el documento\n",
    "    encoded_input = tokenizer(doc, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Pasar el documento tokenizado a través del modelo BERT\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "\n",
    "    # Obtener el vector [CLS] (primera posición del tensor de salida)\n",
    "    cls_embedding = output.last_hidden_state[:, 0, :].squeeze()\n",
    "    return cls_embedding.numpy()\n",
    "\n",
    "# Ejemplo de uso\n",
    "testmessage = \"hola, me llamo antonio y estoy ansioso por comenzar el curso\"\n",
    "bert_cls_vector = make_bert_cls_vec(testmessage, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar los mismos diccionarios y documentos que en el ejemplo con FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios\n",
    "\n",
    "content_test_dictionary = [\n",
    "    \"física\",\n",
    "    \"energía\",\n",
    "    \"átomo\",\n",
    "    \"mecánica\",\n",
    "    \"cuántica\",\n",
    "    \"electromagnetismo\",\n",
    "    \"termodinámica\",\n",
    "    \"relatividad\",\n",
    "    \"partículas\",\n",
    "    \"óptica\",\n",
    "    \"nuclear\",\n",
    "    \"experimento\",\n",
    "    \"teoría\",\n",
    "    \"leyes\",\n",
    "    \"fórmula\",\n",
    "    \"ondas\",\n",
    "    \"electrones\",\n",
    "    \"gravitación\",\n",
    "    \"espectro\",\n",
    "    \"campo\",\n",
    "    \"tarea\",\n",
    "    \"ejercicio\",\n",
    "    \"certamen\",\n",
    "    \"laboratorio\",\n",
    "    \"entrega\", \n",
    "    \"test\",\n",
    "    \"informe\",\n",
    "    \"evaluacion\"\n",
    "    ]\n",
    "presentation_test_dictionary = [\n",
    "    \"gracias\",\n",
    "    \"por favor\",\n",
    "    \"disculpa\",\n",
    "    \"perdon\",\n",
    "    \"bienvenido\",\n",
    "    \"encantado\",\n",
    "    \"amigo\",\n",
    "    \"companero\",\n",
    "    \"equipo\",\n",
    "    \"reunión\",\n",
    "    \"evento\",\n",
    "    \"saludos\",\n",
    "    \"fiesta\",\n",
    "    \"invitación\",\n",
    "    \"sociedad\",\n",
    "    \"familia\",\n",
    "    \"grupo\",\n",
    "    \"encuentro\",\n",
    "    \"hora\",\n",
    "    \"hola\",\n",
    "    \"instagram\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de documentos\n",
    "testmessage1 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso']\n",
    "testmessage2 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage3 = ['física, ondas energia gravitación nuclear óptica átom cuántica']\n",
    "testmessage4 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso. Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage5 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema. La ecuación de Navier-Stokes describe el comportamiento de los fluidos en movimiento. Estas ecuaciones son fundamentales en la dinámica de fluidos y son un conjunto de ecuaciones diferenciales parciales no lineales que describen el flujo de fluidos newtonianos.']\n",
    "\n",
    "doc1_words = preprocess_document(testmessage1)\n",
    "doc2_words = preprocess_document(testmessage2)\n",
    "doc3_words = preprocess_document(testmessage3)\n",
    "doc4_words = preprocess_document(testmessage4)\n",
    "doc5_words = preprocess_document(testmessage5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular los vectores de cada documento y diccionario con el modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que doc1_words, doc2_words, presentation_test_dictionary y content_test_dictionary\n",
    "# sean cadenas de texto o listas de cadenas de texto que representen tus documentos.\n",
    "\n",
    "# Agregación con BERT\n",
    "vec_1_avg_bert = make_bert_vec(' '.join(doc1_words), model, tokenizer)\n",
    "vec_2_avg_bert = make_bert_vec(' '.join(doc2_words), model, tokenizer)\n",
    "vec_dp_avg_bert = make_bert_vec(' '.join(presentation_test_dictionary), model, tokenizer)\n",
    "vec_dc_avg_bert = make_bert_vec(' '.join(content_test_dictionary), model, tokenizer)\n",
    "\n",
    "\n",
    "# Agregación con BERT [CLS]\n",
    "vec_1_avg_bert_cls = make_bert_cls_vec(' '.join(doc1_words), model, tokenizer)\n",
    "vec_2_avg_bert_cls = make_bert_cls_vec(' '.join(doc2_words), model, tokenizer)\n",
    "vec_dp_avg_bert_cls = make_bert_cls_vec(' '.join(presentation_test_dictionary), model, tokenizer)\n",
    "vec_dc_avg_bert_cls = make_bert_cls_vec(' '.join(content_test_dictionary), model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud texto 1 - Presentacion: 0.69377226 -- Similitud texto 1 - Contenido: 0.633451\n",
      "Similitud texto 2 - Presentacion: 0.7217875 -- Similitud texto 2 - Contenido: 0.7161747\n"
     ]
    }
   ],
   "source": [
    "print('Similitud texto 1 - Presentacion:',cos_similarity(vec_1_avg_bert, vec_dp_avg_bert),'--','Similitud texto 1 - Contenido:',cos_similarity(vec_1_avg_bert, vec_dc_avg_bert))\n",
    "print('Similitud texto 2 - Presentacion:',cos_similarity(vec_2_avg_bert, vec_dp_avg_bert),'--','Similitud texto 2 - Contenido:',cos_similarity(vec_2_avg_bert, vec_dc_avg_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud texto 1 - Presentacion: 0.6152363 -- Similitud texto 1 - Contenido: 0.5997947\n",
      "Similitud texto 2 - Presentacion: 0.6791248 -- Similitud texto 2 - Contenido: 0.6764449\n"
     ]
    }
   ],
   "source": [
    "print('Similitud texto 1 - Presentacion:',cos_similarity(vec_1_avg_bert_cls, vec_dp_avg_bert_cls),'--','Similitud texto 1 - Contenido:',cos_similarity(vec_1_avg_bert_cls, vec_dc_avg_bert_cls))\n",
    "print('Similitud texto 2 - Presentacion:',cos_similarity(vec_2_avg_bert_cls, vec_dp_avg_bert_cls),'--','Similitud texto 2 - Contenido:',cos_similarity(vec_2_avg_bert_cls, vec_dc_avg_bert_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'llamar', 'antonio', 'ansioso', 'comenzar', 'curso']\n",
      "['diagrama', 'cuerpo', 'libre', 'permitir', 'visualizar', 'fuerza', 'actuan', 'sistema']\n"
     ]
    }
   ],
   "source": [
    "print(doc1_words)\n",
    "print(doc2_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear \"mini-documentos\" a partir de los diccionarios\n",
    "content_mini_doc = ' '.join(content_test_dictionary)\n",
    "presentation_mini_doc = ' '.join(presentation_test_dictionary)\n",
    "\n",
    "# Obtener vectores BERT para los \"mini-documentos\" y documentos de prueba\n",
    "vec_content = make_bert_cls_vec(content_mini_doc, model, tokenizer)\n",
    "vec_presentation = make_bert_cls_vec(presentation_mini_doc, model, tokenizer)\n",
    "vec_test1 = make_bert_cls_vec(' '.join(doc1_words), model, tokenizer)\n",
    "vec_test2 = make_bert_cls_vec(' '.join(doc2_words), model, tokenizer)\n",
    "\n",
    "# Calcular la similitud (usando, por ejemplo, la similitud del coseno)\n",
    "similarity_test1_content = cos_similarity(vec_test1, vec_content)\n",
    "similarity_test2_content = cos_similarity(vec_test2, vec_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5997947\n",
      "0.6764449\n"
     ]
    }
   ],
   "source": [
    "print(similarity_test1_content)\n",
    "print(similarity_test2_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona mal!!!! Ahora probamos:\n",
    "\n",
    "# S-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install sentence-transformers\n",
    "# %pip3 install pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar un modelo SBERT en español\n",
    "model_name = \"sentence-transformers/stsb-xlm-r-multilingual\" # \n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2044222\n",
      "0.45829985\n",
      "0.8045501\n",
      "0.47069472\n",
      "0.5266916\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de documentos\n",
    "testmessage1 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso']\n",
    "testmessage2 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage3 = ['física, ondas energia gravitación nuclear óptica átom cuántica']\n",
    "testmessage4 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso. Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage5 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema. La ecuación de Navier-Stokes describe el comportamiento de los fluidos en movimiento. Estas ecuaciones son fundamentales en la dinámica de fluidos y son un conjunto de ecuaciones diferenciales parciales no lineales que describen el flujo de fluidos newtonianos.']\n",
    "\n",
    "doc1_words = preprocess_document(testmessage1)\n",
    "doc2_words = preprocess_document(testmessage2)\n",
    "doc3_words = preprocess_document(testmessage3)\n",
    "doc4_words = preprocess_document(testmessage4)\n",
    "doc5_words = preprocess_document(testmessage5)\n",
    "\n",
    "# Obtener vectores de documentos\n",
    "vec_doc1 = model.encode(' '.join(doc1_words))\n",
    "vec_doc2 = model.encode(' '.join(doc2_words))\n",
    "vec_doc3 = model.encode(' '.join(doc3_words))\n",
    "vec_doc4 = model.encode(' '.join(doc4_words))\n",
    "vec_doc5 = model.encode(' '.join(doc5_words))\n",
    "\n",
    "vec_content = model.encode(' '.join(content_test_dictionary))#\n",
    "vec_presentation = model.encode(' '.join(presentation_test_dictionary))\n",
    "\n",
    "# Calcular la similitud (usando, por ejemplo, la similitud del coseno)\n",
    "similarity_test1_content = cos_similarity(vec_doc1, vec_content)\n",
    "similarity_test2_content = cos_similarity(vec_doc2, vec_content)\n",
    "similarity_test3_content = cos_similarity(vec_doc3, vec_content)\n",
    "similarity_test4_content = cos_similarity(vec_doc4, vec_content)\n",
    "similarity_test5_content = cos_similarity(vec_doc5, vec_content)\n",
    "\n",
    "similarity_test1_presentacion = cos_similarity(vec_doc1, vec_presentation)\n",
    "similarity_test2_presentacion = cos_similarity(vec_doc2, vec_presentation)\n",
    "similarity_test3_presentacion = cos_similarity(vec_doc3, vec_presentation)\n",
    "similarity_test4_presentacion = cos_similarity(vec_doc4, vec_presentation)\n",
    "similarity_test5_presentacion = cos_similarity(vec_doc5, vec_presentation)\n",
    "\n",
    "\n",
    "print(similarity_test1_content)\n",
    "print(similarity_test2_content)\n",
    "print(similarity_test3_content)\n",
    "print(similarity_test4_content)\n",
    "print(similarity_test5_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29156387\n",
      "0.06304032\n",
      "0.07182588\n",
      "0.2486218\n",
      "0.05881759\n"
     ]
    }
   ],
   "source": [
    "print(similarity_test1_presentacion)\n",
    "print(similarity_test2_presentacion)\n",
    "print(similarity_test3_presentacion)\n",
    "print(similarity_test4_presentacion)\n",
    "print(similarity_test5_presentacion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir diccionario en frases para S-BERT\n",
    "\n",
    "En lugar de simplemente unir todas las palabras clave en una sola cadena, podríamos tratar de formar oraciones o frases coherentes que hagan un uso significativo de estas palabras. Esto es porque SBERT está optimizado para entender el significado de oraciones completas en lugar de listas de palabras aisladas.\n",
    "\n",
    "# STS-BERT\n",
    "\n",
    "El modelo `stsb-xlm-r-multilingual` es un tipo de modelo de Sentence-BERT (SBERT), específicamente diseñado para tareas de Similitud Semántica de Textos (STS) y es multilingüe. SBERT es una modificación del preentrenado BERT (Bidirectional Encoder Representations from Transformers) para producir incrustaciones (embeddings) de oraciones que pueden ser comparadas eficientemente utilizando la similitud del coseno. Vamos a desglosar las características clave de este modelo:\n",
    "\n",
    "## Características Principales de `stsb-xlm-r-multilingual`:\n",
    "### Sentence-BERT (SBERT):\n",
    "\n",
    "SBERT es una variante de BERT que ha sido modificada y afinada para generar representaciones de oraciones completas, en lugar de tokens individuales. Esto lo hace ideal para tareas donde necesitas comparar la similitud entre oraciones completas o párrafos.\n",
    "\n",
    "### Optimizado para Tareas STS:\n",
    "\n",
    "El modelo ha sido afinado específicamente para tareas de Similitud Semántica de Textos, lo que significa que es muy efectivo en medir cuán similares son dos segmentos de texto en términos de su significado.\n",
    "\n",
    "### Multilingüe:\n",
    "\n",
    "\"XLM-R\" se refiere a \"Cross-lingual Language Model - RoBERTa\", una variante de BERT que ha sido entrenada en varios idiomas. Esto hace que \"stsb-xlm-r-multilingual\" sea efectivo en trabajar con textos en diversos idiomas, incluyendo, pero no limitándose a, el español.\n",
    "Aplicaciones:\n",
    "\n",
    "Este modelo es útil en aplicaciones como la comparación de similitud de oraciones, agrupación semántica, sistemas de recomendación basados en texto y más.\n",
    "\n",
    "### Rendimiento:\n",
    "\n",
    "Debido a su entrenamiento especializado y naturaleza multilingüe, este modelo ofrece un equilibrio entre comprensión del lenguaje en varios idiomas y capacidad para realizar tareas de similitud semántica.\n",
    "\n",
    "### Uso del Modelo:\n",
    "El modelo puede ser utilizado para codificar textos en vectores de tal manera que textos con significados similares estarán cercanos en el espacio vectorial. Esto es particularmente útil para comparar textos en términos de su contenido semántico, independientemente del idioma en que estén escritos.\n",
    "\n",
    "Así, `stsb-xlm-r-multilingual` es una herramienta poderosa y versátil para cualquier aplicación que requiera comprender y comparar el significado semántico de textos en múltiples idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizer\n",
    "# model_name = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n",
    "\n",
    "#Modelo optimizado para la tarea de Similitud Semántica de Textos (STS) y es multilingüe, lo cual es útil si estás trabajando con textos en español.\n",
    "model_name = \"sentence-transformers/stsb-xlm-r-multilingual\" # \n",
    "\n",
    "#Promedio\n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido\n",
      "0.10894067\n",
      "0.39935452\n",
      "0.5988185\n",
      "0.31115535\n",
      "0.48187786\n",
      "Presentacion\n",
      "0.28767073\n",
      "0.24628721\n",
      "0.074146956\n",
      "0.29530466\n",
      "0.12177021\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de términos clave formado en frases coherentes\n",
    "content_phrases = [\n",
    "    \"En mecánica cuántica, la función de onda de Schrödinger describe cómo cambian los estados cuánticos con el tiempo.\",\n",
    "    \"El principio de incertidumbre de Heisenberg postula límites fundamentales en la precisión de las mediciones simultáneas.\",\n",
    "    \"La segunda ley de la termodinámica establece que la entropía total de un sistema aislado siempre aumenta.\",\n",
    "    \"La teoría especial de la relatividad de Einstein revoluciona nuestra comprensión de tiempo y espacio con la ecuación E=mc².\",\n",
    "    \"La ley de Coulomb describe la fuerza eléctrica entre dos cargas puntuales.\",\n",
    "    \"La física de partículas explora el modelo estándar, incluyendo quarks, leptones y bosones de gauge.\",\n",
    "    \"La dualidad onda-partícula es un concepto fundamental en la mecánica cuántica, ilustrando la naturaleza dual de la materia.\",\n",
    "    \"El electromagnetismo se unifica a través de las ecuaciones de Maxwell, que describen cómo los campos eléctricos y magnéticos se propagan y cómo se generan.\",\n",
    "    \"En óptica física, la interferencia y la difracción son fenómenos fundamentales para entender la naturaleza de la luz.\",\n",
    "    \"La física nuclear se enfoca en entender las fuerzas que actúan dentro del núcleo atómico, incluyendo la fuerza fuerte y la fisión nuclear.\",\n",
    "    \"La cosmología física estudia la estructura y evolución del universo, incluyendo teorías sobre el Big Bang y la energía oscura.\",\n",
    "    \"Los experimentos en física de alta energía, como los realizados en el Gran Colisionador de Hadrones, buscan descubrir nuevas partículas.\",\n",
    "    \"La mecánica estadística conecta las propiedades macroscópicas de los sistemas con las descripciones microscópicas de sus componentes.\",\n",
    "    \"La superconductividad y los fenómenos de estados cuánticos de la materia ofrecen perspectivas para avances tecnológicos.\"\n",
    "]\n",
    "\n",
    "\n",
    "presentation_phrases = [\n",
    "    \"Gracias por su atención y por favor siéntanse libres de hacer preguntas.\",\n",
    "    \"Me disculpo de antemano por cualquier error y pido perdón por las molestias.\",\n",
    "    \"Bienvenidos a este encuentro donde estaremos encantados de compartir ideas.\",\n",
    "    \"Amigos y compañeros, unamos esfuerzos para trabajar en equipo.\",\n",
    "    \"Esta reunión será crucial para planificar el próximo evento social.\",\n",
    "    \"Saludos a todos, espero que disfruten de la fiesta y la invitación.\",\n",
    "    \"La sociedad se beneficia cuando familias y grupos colaboran en encuentros.\",\n",
    "    \"La hora de inicio será puntual, así que les digo hola y bienvenidos.\",\n",
    "    \"Síganos en Instagram para más actualizaciones sobre futuros eventos.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Codificar las frases del diccionario\n",
    "vec_content_phrases = [model.encode(phrase) for phrase in content_phrases]\n",
    "vec_presentacion_phrases = [model.encode(phrase) for phrase in presentation_phrases]\n",
    "\n",
    "\n",
    "# Ejemplo de documentos\n",
    "testmessage1 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso']\n",
    "testmessage2 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage3 = ['física, ondas energia gravitación nuclear óptica átom cuántica']\n",
    "testmessage4 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso. Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage5 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema. La ecuación de Navier-Stokes describe el comportamiento de los fluidos en movimiento. Estas ecuaciones son fundamentales en la dinámica de fluidos y son un conjunto de ecuaciones diferenciales parciales no lineales que describen el flujo de fluidos newtonianos.']\n",
    "\n",
    "doc1_words = preprocess_document(testmessage1)\n",
    "doc2_words = preprocess_document(testmessage2)\n",
    "doc3_words = preprocess_document(testmessage3)\n",
    "doc4_words = preprocess_document(testmessage4)\n",
    "doc5_words = preprocess_document(testmessage5)\n",
    "\n",
    "# Obtener vectores de documentos\n",
    "vec_doc1 = model.encode(' '.join(doc1_words))\n",
    "vec_doc2 = model.encode(' '.join(doc2_words))\n",
    "vec_doc3 = model.encode(' '.join(doc3_words))\n",
    "vec_doc4 = model.encode(' '.join(doc4_words))\n",
    "vec_doc5 = model.encode(' '.join(doc5_words))\n",
    "\n",
    "# Calcular la similitud promedio entre el documento y cada frase del diccionario\n",
    "average_similarity1 = np.mean([cos_similarity(vec_doc1, vec_phrase) for vec_phrase in vec_content_phrases])\n",
    "average_similarity2 = np.mean([cos_similarity(vec_doc2, vec_phrase) for vec_phrase in vec_content_phrases])\n",
    "average_similarity3 = np.mean([cos_similarity(vec_doc3, vec_phrase) for vec_phrase in vec_content_phrases])\n",
    "average_similarity4 = np.mean([cos_similarity(vec_doc4, vec_phrase) for vec_phrase in vec_content_phrases])\n",
    "average_similarity5 = np.mean([cos_similarity(vec_doc5, vec_phrase) for vec_phrase in vec_content_phrases])\n",
    "\n",
    "average_similarity1_pres = np.mean([cos_similarity(vec_doc1, vec_phrase) for vec_phrase in vec_presentacion_phrases])\n",
    "average_similarity2_pres = np.mean([cos_similarity(vec_doc2, vec_phrase) for vec_phrase in vec_presentacion_phrases])\n",
    "average_similarity3_pres = np.mean([cos_similarity(vec_doc3, vec_phrase) for vec_phrase in vec_presentacion_phrases])\n",
    "average_similarity4_pres = np.mean([cos_similarity(vec_doc4, vec_phrase) for vec_phrase in vec_presentacion_phrases])\n",
    "average_similarity5_pres = np.mean([cos_similarity(vec_doc5, vec_phrase) for vec_phrase in vec_presentacion_phrases])\n",
    "\n",
    "\n",
    "print('Contenido')\n",
    "print(average_similarity1)\n",
    "print(average_similarity2)\n",
    "print(average_similarity3)\n",
    "print(average_similarity4)\n",
    "print(average_similarity5)\n",
    "print('Presentacion')\n",
    "print(average_similarity1_pres)\n",
    "print(average_similarity2_pres)\n",
    "print(average_similarity3_pres)\n",
    "print(average_similarity4_pres)\n",
    "print(average_similarity5_pres)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
