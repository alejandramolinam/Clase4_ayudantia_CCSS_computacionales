{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL PACKAGES: \n",
    "\n",
    "#%pip install pandas\n",
    "#%pip install numpy\n",
    "#%pip install numpyencoder \n",
    "#%pip install unicodedata \n",
    "#%pip install datetime\n",
    "#%pip install matplotlib\n",
    "#%pip install scikit-learn\n",
    "#%pip install scipy\n",
    "#%pip install nltk\n",
    "#%pip install operator\n",
    "#%pip install gensim\n",
    "#%pip install spacy\n",
    "#%pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de DDR con Fast Text\n",
    "\n",
    "Si quieren leer más, DDR está super bien explicado en este sitio:\n",
    "https://github.com/USC-CSSL/DDR/blob/master/docs/DDR-Tutorial.rst#generate-document-dictionary-similarity-measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##imports##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from random import sample \n",
    "from numpyencoder import NumpyEncoder\n",
    "import math\n",
    "from unicodedata import normalize\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend(['i','ii','iii','iv','v','vi','vii', 'ix' , 'x' , 'xi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "nlp  =  spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cargar el modelo de Word Embeddings ya entrenado en español por FastText con textos de wikipedia (Esto puede consumir harta memoria, pq el archivo pesa más de 2Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model Loading##\n",
    "# embeddings-l-model.vec: Spanish Unannotated Corpora\t2.6B\tFastText\t1,313,423\t300 José Cañete\n",
    "# wiki.es.vec:            Spanish Wikipedia\t             ???\tFastText\t985,667\t    300\tFastText team\n",
    "\n",
    "wordvectors_file_vec  =  'word_embeddings_models/wiki.es.vec'#'embeddings-l-model.vec'#\n",
    "wordvectors  =  KeyedVectors.load_word2vec_format(wordvectors_file_vec)\n",
    "model_word_set = list(wordvectors.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir una serie de funciones que serán útiles para procesar el texto (sacar stop-words, lemmatizar y esas cosas), para calcular los vectores representativos de un documento y para hacer los cálculos de similitud del coseno entre dos vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function definitions##\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True,max_len = 22))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc),max_len = 22,deacc = True) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV','PROPN','X']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out  =  []\n",
    "    for sent in texts:\n",
    "        doc  =  nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def make_agg_vec(words, model, num_features, model_word_set, filter_out = []):\n",
    "    \"\"\"Create aggregate representation of list of words\"\"\"\n",
    "    feature_vec  =  np.zeros((num_features,), dtype = \"float32\")\n",
    "    nwords  =  0.0\n",
    "    lost_words = []\n",
    "    for word in words:\n",
    "        if word not in filter_out:\n",
    "            if word in model_word_set:\n",
    "                nwords += 1\n",
    "                feature_vec = np.add(feature_vec , model[word])\n",
    "            else:\n",
    "                lost_words.append(word)\n",
    "    if nwords>0:\n",
    "        avg_feature_vec = feature_vec / nwords\n",
    "        return [avg_feature_vec,lost_words]\n",
    "    else:\n",
    "      return [feature_vec,lost_words]\n",
    "\n",
    "\n",
    "def cos_similarity_distance(v1, v2):\n",
    " return 1-cos_similarity(v1,v2)\n",
    "\n",
    "\n",
    "\n",
    "def cos_similarity_angle(v1, v2):\n",
    "    prod  =  np.dot(v1, v2)\n",
    "    len1  =  math.sqrt(np.dot(v1, v1))\n",
    "    len2  =  math.sqrt(np.dot(v2, v2))\n",
    "    if   abs(prod / (len1 * len2) -1)<(10**(-4)):\n",
    "        return 0\n",
    "    if len1*len2 == 0:\n",
    "        return -1\n",
    "    else:\n",
    "      return   math.degrees(    math.acos( prod / (len1 * len2))  )# retorna un angulo entre 0 y 180\n",
    "    \n",
    "\n",
    "\n",
    "def cos_similarity_pearson(v1, v2):\n",
    "    v1    = v1-np.mean(v1)\n",
    "    v2    = v2-np.mean(v2)\n",
    "    prod  =  np.dot(v1, v2)\n",
    "    len1  =  math.sqrt(np.dot(v1, v1))\n",
    "    len2  =  math.sqrt(np.dot(v2, v2))\n",
    "    if len1*len2 == 0:\n",
    "        return -1\n",
    "    else:\n",
    "      return   prod / (len1 * len2)  # retorna un angulo entre 0 y 180\n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    \"\"\" Preprocesar el documento eliminando stopwords, lematizando, eliminando caracteres no alfanuméricos y palabras vacías. \"\"\"\n",
    "    # Asegurarse de que el documento sea una cadena\n",
    "    if isinstance(doc, list):\n",
    "        doc = ' '.join(doc)\n",
    "    # Eliminar stopwords y lematizar\n",
    "    # doc = nlp(doc)\n",
    "    # lemmatized = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # print(doc)\n",
    "    no_stopw=remove_stopwords([doc])\n",
    "    # print(no_stopw)\n",
    "    lemmatized=lemmatization(no_stopw)\n",
    "    # Eliminar caracteres no alfanuméricos, convertir a minúsculas y eliminar palabras vacías\n",
    "    # cleaned = [re.sub(r'\\W+', '', word).lower() for word in lemmatized]\n",
    "    cleaned = [word for word in lemmatized]  # Elimina las palabras vacías\n",
    "    cleaned=cleaned[0]\n",
    "    return cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a definir los diccionarios con los conceptos \"Contenido\" y \"Presentación\" cada uno representado por una serie de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios\n",
    "\n",
    "content_test_dictionary = [\n",
    "    \"física\",\n",
    "    \"energía\",\n",
    "    \"átomo\",\n",
    "    \"mecánica\",\n",
    "    \"cuántica\",\n",
    "    \"electromagnetismo\",\n",
    "    \"termodinámica\",\n",
    "    \"relatividad\",\n",
    "    \"partículas\",\n",
    "    \"óptica\",\n",
    "    \"nuclear\",\n",
    "    \"experimento\",\n",
    "    \"teoría\",\n",
    "    \"leyes\",\n",
    "    \"fórmula\",\n",
    "    \"ondas\",\n",
    "    \"electrones\",\n",
    "    \"gravitación\",\n",
    "    \"espectro\",\n",
    "    \"campo\",\n",
    "    \"tarea\",\n",
    "    \"ejercicio\",\n",
    "    \"certamen\",\n",
    "    \"laboratorio\",\n",
    "    \"entrega\", \n",
    "    \"test\",\n",
    "    \"informe\",\n",
    "    \"evaluacion\"\n",
    "    ]\n",
    "presentation_test_dictionary = [\n",
    "    \"gracias\",\n",
    "    \"por favor\",\n",
    "    \"disculpa\",\n",
    "    \"perdon\",\n",
    "    \"bienvenido\",\n",
    "    \"encantado\",\n",
    "    \"amigo\",\n",
    "    \"companero\",\n",
    "    \"equipo\",\n",
    "    \"reunión\",\n",
    "    \"evento\",\n",
    "    \"saludos\",\n",
    "    \"fiesta\",\n",
    "    \"invitación\",\n",
    "    \"sociedad\",\n",
    "    \"familia\",\n",
    "    \"grupo\",\n",
    "    \"encuentro\",\n",
    "    \"hora\",\n",
    "    \"hola\",\n",
    "    \"instagram\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos las siguientes frases para clasificarlas entre \"Contenido\" y \"Presentación\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de documentos\n",
    "testmessage1 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso']\n",
    "testmessage2 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage3 = ['física, ondas energia gravitación nuclear óptica átom cuántica']\n",
    "testmessage4 = ['hola, me llamo antonio y estoy ansioso por comenzar el curso. Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
    "testmessage5 = ['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema. La ecuación de Navier-Stokes describe el comportamiento de los fluidos en movimiento. Estas ecuaciones son fundamentales en la dinámica de fluidos y son un conjunto de ecuaciones diferenciales parciales no lineales que describen el flujo de fluidos newtonianos.']\n",
    "\n",
    "doc1_words = preprocess_document(testmessage1)\n",
    "doc2_words = preprocess_document(testmessage2)\n",
    "doc3_words = preprocess_document(testmessage3)\n",
    "doc4_words = preprocess_document(testmessage4)\n",
    "doc5_words = preprocess_document(testmessage5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular el vector que representa a cada uno de los documentos, usando el modelo de word embeddings en cada una de las palabras del documento y promediándolas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregación Promedio: wordvectors es el modelo de Fast Text (los vectores) y model_word_set es el conjunto de palabras del modelo\n",
    "vec_1_avg = make_agg_vec(doc1_words, wordvectors, 300, model_word_set)[0]\n",
    "vec_2_avg = make_agg_vec(doc2_words, wordvectors, 300, model_word_set)[0]\n",
    "vec_3_avg = make_agg_vec(doc3_words, wordvectors, 300, model_word_set)[0]\n",
    "vec_4_avg = make_agg_vec(doc4_words, wordvectors, 300, model_word_set)[0]\n",
    "vec_5_avg = make_agg_vec(doc5_words, wordvectors, 300, model_word_set)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer lo mismo para cada concepto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dp_avg = make_agg_vec(presentation_test_dictionary, wordvectors, 300, model_word_set)[0]\n",
    "vec_dc_avg = make_agg_vec(content_test_dictionary, wordvectors, 300, model_word_set)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular que tan cerca o lejos están cada documento de cada concepto, calculando el coseno del ángulo entre los vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la similitud (usando, por ejemplo, la similitud del coseno)\n",
    "similarity_test1_content = cos_similarity(vec_1_avg, vec_dc_avg)\n",
    "similarity_test2_content = cos_similarity(vec_2_avg, vec_dc_avg)\n",
    "similarity_test3_content = cos_similarity(vec_3_avg, vec_dc_avg)\n",
    "similarity_test4_content = cos_similarity(vec_4_avg, vec_dc_avg)\n",
    "similarity_test5_content = cos_similarity(vec_5_avg, vec_dc_avg)\n",
    "\n",
    "similarity_test1_presentacion = cos_similarity(vec_1_avg, vec_dp_avg)\n",
    "similarity_test2_presentacion = cos_similarity(vec_2_avg, vec_dp_avg)\n",
    "similarity_test3_presentacion = cos_similarity(vec_3_avg, vec_dp_avg)\n",
    "similarity_test4_presentacion = cos_similarity(vec_4_avg, vec_dp_avg)\n",
    "similarity_test5_presentacion = cos_similarity(vec_5_avg, vec_dp_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola, me llamo antonio y estoy ansioso por comenzar el curso']\n",
      "Similitud texto 1 - Presentacion: 0.7337279 -- Similitud texto 1 - Contenido: 0.45199558\n",
      "['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
      "Similitud texto 2 - Presentacion: 0.4917478 -- Similitud texto 2 - Contenido: 0.67510384\n",
      "['física, ondas energia gravitación nuclear óptica átom cuántica']\n",
      "Similitud texto 2 - Presentacion: 0.43267116 -- Similitud texto 2 - Contenido: 0.7785211\n",
      "['hola, me llamo antonio y estoy ansioso por comenzar el curso. Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema']\n",
      "Similitud texto 2 - Presentacion: 0.680647 -- Similitud texto 2 - Contenido: 0.664385\n",
      "['Los diagramas de cuerpo libre nos permiten visualizar que fuerzas actuan sobre un sistema. La ecuación de Navier-Stokes describe el comportamiento de los fluidos en movimiento. Estas ecuaciones son fundamentales en la dinámica de fluidos y son un conjunto de ecuaciones diferenciales parciales no lineales que describen el flujo de fluidos newtonianos.']\n",
      "Similitud texto 2 - Presentacion: 0.45084357 -- Similitud texto 2 - Contenido: 0.808466\n"
     ]
    }
   ],
   "source": [
    "print(testmessage1)\n",
    "print('Similitud texto 1 - Presentacion:',cos_similarity(vec_1_avg, vec_dp_avg),'--','Similitud texto 1 - Contenido:',cos_similarity(vec_1_avg, vec_dc_avg))\n",
    "print(testmessage2)\n",
    "print('Similitud texto 2 - Presentacion:',cos_similarity(vec_2_avg, vec_dp_avg),'--','Similitud texto 2 - Contenido:',cos_similarity(vec_2_avg, vec_dc_avg))\n",
    "print(testmessage3)\n",
    "print('Similitud texto 2 - Presentacion:',cos_similarity(vec_3_avg, vec_dp_avg),'--','Similitud texto 2 - Contenido:',cos_similarity(vec_3_avg, vec_dc_avg))\n",
    "print(testmessage4)\n",
    "print('Similitud texto 2 - Presentacion:',cos_similarity(vec_4_avg, vec_dp_avg),'--','Similitud texto 2 - Contenido:',cos_similarity(vec_4_avg, vec_dc_avg))\n",
    "print(testmessage5)\n",
    "print('Similitud texto 2 - Presentacion:',cos_similarity(vec_5_avg, vec_dp_avg),'--','Similitud texto 2 - Contenido:',cos_similarity(vec_5_avg, vec_dc_avg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
